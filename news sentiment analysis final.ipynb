{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment Analysis of English Newspaper Articles \n",
    "\n",
    "This code analyzes the sentiment of English newspaper articles from Hankyoreh, Joongang, ABC News, and Fox News on the COVID-19 Pandemic around 2020. \n",
    "\n",
    "Basic Steps of the code are:\n",
    " - Step 1. Instantiate Model\n",
    " - Step 2. Calculate Sentiment\n",
    " - Step 3. Visualization & Summary of Results\n",
    "\n",
    "This code is written by Hyebin Seo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1. Import Libraries & Read in Excel Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\seohy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df(path):\n",
    "    \"\"\"\n",
    "    Loads an Excel file from the specified path into a pandas DataFrame.\n",
    "\n",
    "    :param path: String representing the file path to the Excel file.\n",
    "    :return: A pandas DataFrame containing the data from the Excel file.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_excel(path)\n",
    "    # fmt: off\n",
    "    df = df.rename(columns={\n",
    "        \"Country\": \"country\",\n",
    "        \"신문사\": \"newspaper\", \n",
    "        \"제목\": \"title\", \n",
    "        \"본문\": \"body\", \n",
    "        \"URL\": \"url\"})\n",
    "    # fmt: on\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = input(\"Please input path to excel file(s) separated with commas: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv data at path(s)\n",
    "\n",
    "if \",\" in paths:\n",
    "    paths = paths.split(\",\")\n",
    "    raw_df = pd.concat([read_df(path) for path in paths])\n",
    "else:\n",
    "    raw_df = read_df(paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Step 2. Search for Sentences with Target Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = input(\"Enter a word to find its sentiment: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentence(df, column=\"body\"):\n",
    "    \"\"\"\n",
    "    Splits text in specified column of a dataframe into sentences\n",
    "\n",
    "    :param df: DataFrame with a column containing text to be split\n",
    "    :param column: The name of the column in DataFrame that contains text to be split into sentences\n",
    "    :return: A new DataFrame where each row corresponds to a sentence from the text in 'column'\n",
    "    \"\"\"\n",
    "\n",
    "    df[column] = df[column].apply(sent_tokenize)\n",
    "\n",
    "    split_df = df.explode(column, ignore_index=True)\n",
    "    \n",
    "    return split_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_with_target(df, column=\"body\", word=target_word):\n",
    "    \"\"\"\n",
    "    Returns a dataframe of news sentences containing input word\n",
    "\n",
    "    :param df: a dataframe containing sentences from article\n",
    "    :param column: a column to search for input word\n",
    "    :param word: the target word to search for\n",
    "    :return: a dataframe of sentences from the dataframe\n",
    "    \"\"\"\n",
    "    mask = df[column].str.contains(target_word, na=False, case=False) \n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = sentences_with_target(split_into_sentence(raw_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df[\"body\"] = filtered_df[\"body\"].replace({'masks': 'mask'}, regex=True)\n",
    "filtered_df[\"body\"] = filtered_df[\"body\"].replace({'Masks': 'mask'}, regex=True)\n",
    "filtered_df[\"body\"] = filtered_df[\"body\"].replace({'Mask': 'mask'}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>Seoul’s subway app will soon enable users to r...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>The Seoul Metropolitan Government announced th...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>When such a report is made, security staff wil...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>Seoul plans to issue fines and take other ster...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>Even though it’s now mandatory to wear mask on...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>From May 13, when Seoul implemented “daily soc...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>There were also five cases in which subway emp...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>During the same period, city buses have seen a...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>“Many subway passengers who see someone not we...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>[Reportage] Behind the scenes of S. Korea’s di...</td>\n",
       "      <td>Behind their mask, their faces showed signs of...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  newspaper                                              title  \\\n",
       "0    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "1    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "2    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "3    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "4    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "6    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "7    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "9    Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "10   Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "19   Korea  Hankyoreh  [Reportage] Behind the scenes of S. Korea’s di...   \n",
       "\n",
       "                                                 body  \\\n",
       "0   Seoul’s subway app will soon enable users to r...   \n",
       "1   The Seoul Metropolitan Government announced th...   \n",
       "2   When such a report is made, security staff wil...   \n",
       "3   Seoul plans to issue fines and take other ster...   \n",
       "4   Even though it’s now mandatory to wear mask on...   \n",
       "6   From May 13, when Seoul implemented “daily soc...   \n",
       "7   There were also five cases in which subway emp...   \n",
       "9   During the same period, city buses have seen a...   \n",
       "10  “Many subway passengers who see someone not we...   \n",
       "19  Behind their mask, their faces showed signs of...   \n",
       "\n",
       "                                                  url  \n",
       "0   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "1   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "2   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "3   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "4   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "6   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "7   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "9   https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "10  https://www.hani.co.kr/arti/english_edition/e_...  \n",
       "19  https://www.hani.co.kr/arti/english_edition/e_...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3. Run Semantic Analysis using NewsMTSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm\n",
    "from NewsSentiment import TargetSentimentClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = TargetSentimentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewsSentiment.customexceptions import TooLongTextException, TargetNotFoundException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(sentences, word=target_word):\n",
    "    \"\"\"\n",
    "    Returns a dataframe of the results of sentiment analysis\n",
    "\n",
    "    :param sentences: the sentences to run sentiment analysis on\n",
    "    :param word: the target word to run a sentiment analysis on\n",
    "    :return: a dataframe of the results of sentiment analysis\n",
    "    \"\"\"\n",
    "    left_of_word, match, right_of_word = sentences.rpartition(word) # multiple words in same sentence?, check rpartition\n",
    "\n",
    "    try:\n",
    "        result = model.infer_from_text(left_of_word, match, right_of_word)\n",
    "        df = pd.DataFrame(result)\n",
    "        classification_result = df.loc[\n",
    "            df[\"class_prob\"].idxmax()\n",
    "        ]  # get the row with the highest probability\n",
    "        return classification_result\n",
    "    \n",
    "    except TargetNotFoundException:\n",
    "        print(\"Trarget word was not found in the text. Perhaps the target is in a different case? Perhaps you misspelled it?\")\n",
    "\n",
    "    except TooLongTextException:\n",
    "        print(\"Text is too long, split the text into smaller chunks\")  \n",
    "        return pd.Series([None] * 3)\n",
    "\n",
    "    except Exception: # could not figure it out # log 작성하기(?), exception에 대한 근거 \n",
    "                      # 한 번 더 다루어보기\n",
    "                      # DE → make developer 확인하기, ex) print에서 확인하기\n",
    "        print(\"An error has occured\")\n",
    "        return pd.Series([None] * 3)  # error for now, fix later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_result(df):\n",
    "    \"\"\"\n",
    "    Returns a combined dataframe of the original and sentiment analysis results\n",
    "\n",
    "    :param df: a dataframe of the original dataframe of news articles\n",
    "    :return: a combined dataframe of the original and sentiment analysis results\n",
    "    \"\"\"\n",
    "\n",
    "    result_df = df[\"body\"].apply(sentiment_analysis)\n",
    "\n",
    "    final_df = pd.concat([df, result_df], axis=1)\n",
    "    final_df = final_df.drop(labels=[0, 1, 2], axis=1)\n",
    "    final_df = final_df.dropna()\n",
    "    final_df = final_df.reset_index(drop=True)\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\seohy\\anaconda3\\envs\\NewsSentiment\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2310: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is too long, splitting into smaller chunks\n",
      "An error has occured\n",
      "An error has occured\n",
      "An error has occured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "raw_result = all_result(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>newspaper</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>url</th>\n",
       "      <th>class_id</th>\n",
       "      <th>class_label</th>\n",
       "      <th>class_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>Seoul’s subway app will soon enable users to r...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.604794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Korea</td>\n",
       "      <td>Hankyoreh</td>\n",
       "      <td>Seoul’s subway app to allow users to report pa...</td>\n",
       "      <td>The Seoul Metropolitan Government announced th...</td>\n",
       "      <td>https://www.hani.co.kr/arti/english_edition/e_...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.639253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  newspaper                                              title  \\\n",
       "0   Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "1   Korea  Hankyoreh  Seoul’s subway app to allow users to report pa...   \n",
       "\n",
       "                                                body  \\\n",
       "0  Seoul’s subway app will soon enable users to r...   \n",
       "1  The Seoul Metropolitan Government announced th...   \n",
       "\n",
       "                                                 url  class_id class_label  \\\n",
       "0  https://www.hani.co.kr/arti/english_edition/e_...       1.0     neutral   \n",
       "1  https://www.hani.co.kr/arti/english_edition/e_...       1.0     neutral   \n",
       "\n",
       "   class_prob  \n",
       "0    0.604794  \n",
       "1    0.639253  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_result.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4. Filter Results of Semantic Analysis to Export into One CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_result(df):\n",
    "    \"\"\"\n",
    "    Returns a dataframe where the probability of sentiment analysis is higher than 0.7\n",
    "\n",
    "    :param df: the combined df returned from function all_result\n",
    "    :return: a dataframe where the probability of sentiment analysis is greater than 0.7\n",
    "    \"\"\"\n",
    "    result_df = df[df[\"class_prob\"] > 0.7]\n",
    "    return result_df.reset_index(drop=True)  # reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combines and exports results of all newspapers into one csv file\n",
    "\n",
    "final_results = filter_result(raw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results.to_excel(\"results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 5. Run a Chi-Sqaured Test on Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mask'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contingency_table(df, index_name=\"class_label\", column_name=\"country\"):\n",
    "    \"\"\"\n",
    "    Returns a contingency table of the results of sentiment analysis\n",
    "\n",
    "    :param df: a dataframe of the results of sentiment analysis\n",
    "    :param column_name: the index for the contingency table\n",
    "    :param column_name: the column for the contingency table\n",
    "    :param word: the target word to run a sentiment analysis on\n",
    "    :return: a contingency table of the results of sentiment analysis\n",
    "    \"\"\"\n",
    "\n",
    "    contingency_table = pd.crosstab(df[index_name], df[column_name])\n",
    "\n",
    "    return contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_sqaure_test(contingency_table):\n",
    "    \"\"\"\n",
    "    Prints the results of a chi-square test\n",
    "\n",
    "    :param contingency_table: a contingency table to run chi-square test on\n",
    "\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "    residuals = (contingency_table - expected) / np.sqrt(expected)\n",
    "\n",
    "    print(f\"Result of comparison:\\n\")\n",
    "\n",
    "    print(f\"Chi-squared Statistic: {chi2}\")\n",
    "\n",
    "    print(f\"Degree of freedom: {dof}\")\n",
    "\n",
    "    print(f\"Expected:{expected}\\n\")\n",
    "\n",
    "    print(f\"P-value: {p}\")\n",
    "\n",
    "    print(f\"Residuals: {residuals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>country</th>\n",
       "      <th>Korea</th>\n",
       "      <th>US</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class_label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>28</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>68</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "country      Korea  US\n",
       "class_label           \n",
       "negative        28  95\n",
       "neutral         68  83\n",
       "positive        11  23"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contingency_table(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of comparison:\n",
      "\n",
      "Chi-squared Statistic: 14.922966447485889\n",
      "Degree of freedom: 2\n",
      "Expected:[[42.73051948 80.26948052]\n",
      " [52.45779221 98.54220779]\n",
      " [11.81168831 22.18831169]]\n",
      "\n",
      "P-value: 0.0005748029774811731\n",
      "Residuals: country         Korea        US\n",
      "class_label                    \n",
      "negative    -2.253455  1.644155\n",
      "neutral      2.145891 -1.565675\n",
      "positive    -0.236175  0.172317\n"
     ]
    }
   ],
   "source": [
    "chi_sqaure_test(contingency_table(final_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NewsSentiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
